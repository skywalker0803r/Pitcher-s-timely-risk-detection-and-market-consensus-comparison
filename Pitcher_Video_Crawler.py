"""
Shohei_Ohtani (å¤§è°·ç¿”å¹³)
Yu_Darvish (é”æ¯”ä¿®æœ‰)
Gerrit_Cole 
çƒç¨®:
FF = å››ç¸«ç·š
SL = æ»‘çƒ
FS = å¿«é€ŸæŒ‡å‰çƒ
CH = è®Šé€Ÿ
"""


#===è¼‰å…¥å¿…è¦å¥—ä»¶===
import os       #os è™•ç†æª”æ¡ˆç³»çµ±ï¼ˆå»ºç«‹è³‡æ–™å¤¾ç­‰ï¼‰
import time     #time ç”¨ä¾†è¨­å®šç­‰å¾…æ™‚é–“
import requests     #requests ç™¼ HTTP è«‹æ±‚ï¼ˆä¸‹è¼‰å½±ç‰‡ç”¨ï¼‰
from bs4 import BeautifulSoup   #BeautifulSoup è§£æ HTML
from selenium import webdriver  #selenium ç”¨æ–¼è‡ªå‹•æ“ä½œç¶²é 
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC



#=== åˆå§‹åŒ–èˆ‡è¨­å®š ===
#æ”¾ç½®å½±é¢ä¸‹è¼‰çš„è³‡æ–™å¤¾åç¨±ï¼Œè‹¥æ²’æœ‰æœƒè‡ªå‹•æ–°å¢
# output_dir = "Gerrit_Cole_ff_videos"    #Gerrit_Cole FF
# output_dir = "Gerrit_Cole_SL_videos"    #Gerrit_Cole SL
# output_dir = "Gerrit_Cole_CH_videos"    #Gerrit_Cole CH
# output_dir = "Shohei_Ohtani_ff_videos"  #Shohei_Ohtani FF
output_dir = "Shohei_Ohtani_SL_videos"  #Shohei_Ohtani SL
# output_dir = "Shohei_Ohtani_FS_videos"  #Shohei_Ohtani FS
# output_dir = "Yu_Darvish_ff_videos"     #Yu_Darvish FF
# output_dir = "Yu_Darvish_SL_videos"     #Yu_Darvish SL
# output_dir = "Yu_Darvish_FS_videos"     #Yu_Darvish FS
os.makedirs(output_dir, exist_ok=True)

#===è¨­å®š Chrome ç€è¦½å™¨å•Ÿå‹•é¸é …ä¸¦é–‹å•Ÿ===
options = webdriver.ChromeOptions()
options.add_argument('--start-maximized')
driver = webdriver.Chrome(options=options)

# è¼¸å…¥éœ€è¦æˆªå–çš„ç¶²é é é¢
# search_url = "https://reurl.cc/RY7r86" #Gerrit_Cole FF
# search_url = "https://reurl.cc/DqMyY5" #Gerrit_Cole SL
# search_url = "https://reurl.cc/knb1kr" #Gerrit_Cole CH
# search_url = "https://reurl.cc/RY7rEg" #Shohei_Ohtani FF
search_url = "https://reurl.cc/AMW7zQ" #Shohei_Ohtani SL
# search_url = "https://reurl.cc/M3m0L4" #Shohei_Ohtani FS
# search_url = "https://reurl.cc/6K4Z3O" #Yu_Darvish FF
# search_url = "https://reurl.cc/aebnjZ" #Yu_Darvish SL
# search_url = "https://reurl.cc/nmb1dD" #Yu_Darvish FS
driver.get(search_url)
time.sleep(15)


# é»æ“Šå±•é–‹æŒ‡å®šæ¬„ä½
try:
    td_to_click = WebDriverWait(driver, 20).until(
        # EC.element_to_be_clickable((By.XPATH, "//*[@id='player_name_543037_']/td[5]")) #Gerrit_Cole
        EC.element_to_be_clickable((By.XPATH, "//*[@id='player_name_660271_']/td[5]")) #Shohei_Ohtani
        # EC.element_to_be_clickable((By.XPATH, "//*[@id='player_name_506433_']/td[5]")) #Yu_Darvish
    )
    td_to_click.click()
    print("âœ… æˆåŠŸé»æ“Šå±•é–‹æ¬„ä½")
    time.sleep(3)
except Exception as e:
    print(f"âŒ ç„¡æ³•é»æ“Šå±•é–‹æ¬„ä½: {e}")
    driver.quit()
    exit()

# ===è‡ªå‹•å‘ä¸‹æ²å‹•è‡³é é¢åº•éƒ¨ï¼Œç¢ºä¿è¼‰å…¥å…¨éƒ¨è³‡æ–™===
# ===ä¸æ–·æ²å‹•ç¶²é ç›´åˆ°ç„¡æ–°å…§å®¹ç‚ºæ­¢ï¼Œç”¨ä¾†è§¸ç™¼å‹•æ…‹è¼‰å…¥===
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

print("ğŸ“¥ é–‹å§‹æ“·å–å½±ç‰‡é€£çµ...")

# ===æŠ“å–å½±ç‰‡æ’­æ”¾é é¢é€£çµ===
# æ“·å–å½±ç‰‡é€£çµï¼ˆç¬¬15æ¬„ a[href*='sporty-videos']ï¼‰
video_links = driver.find_elements(By.XPATH, "//table[contains(@id, 'ajaxTable')]/tbody/tr/td[15]/a[contains(@href, 'sporty-videos?playId=')]")
urls = [link.get_attribute("href") for link in video_links if link.get_attribute("href")][:200] #:æŠ“200ç­†è³‡æ–™

# === è³‡æ–™æŠ“å–å®Œç•¢ï¼Œé—œé–‰è‡ªå‹•åŒ–çš„ Chrome ç€è¦½å™¨===
driver.quit()
print(f"âœ… å…±æ“·å– {len(urls)} ç­†å½±ç‰‡é€£çµï¼Œé–‹å§‹ä¸‹è¼‰å½±ç‰‡...")

# å½±ç‰‡ä¸‹è¼‰å€
for i, url in enumerate(urls):  #å°æ¯å€‹å½±ç‰‡æ’­æ”¾é é¢ç™¼é€ HTTP è«‹æ±‚
    try:
        print(f"[{i+1}/{len(urls)}] é–‹å•Ÿå½±ç‰‡é é¢: {url}")
        r = requests.get(url)
        if r.status_code != 200:
            print(f"âŒ ç„¡æ³•æ‰“é–‹å½±ç‰‡é é¢ï¼š{url}")
            continue

        soup = BeautifulSoup(r.text, 'html.parser')     #ç”¨ BeautifulSoup è§£æå½±ç‰‡ HTML é é¢ï¼ŒæŠ“å‡º <video><source src="..."></source></video> ä¸­çš„å½±ç‰‡ç¶²å€ã€‚
        video_tag = soup.find("video")
        if not video_tag:
            print("âŒ æ‰¾ä¸åˆ° <video> tag")
            continue
        source = video_tag.find("source")
        if not source or not source.get("src"):
            print("âŒ æ‰¾ä¸åˆ°å½±ç‰‡ä¾†æº")
            continue

        mp4_url = source["src"]
        filename = f"pitch_{i+1:04d}.mp4"   #ä¾ç…§ç·¨è™Ÿå‘½åå½±ç‰‡ï¼ˆå¦‚ pitch_0001.mp4ï¼‰
        filepath = os.path.join('train_data',output_dir, filename)

        if os.path.exists(filepath):    #è‹¥å½±ç‰‡å·²å­˜åœ¨å‰‡è·³é
            print(f"â­ æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éï¼š{filename}")
            continue

        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è¼‰: {mp4_url}")
        with requests.get(mp4_url, stream=True) as vid:     #ä¸²æµæ–¹å¼ä¸‹è¼‰å½±ç‰‡ï¼Œé¿å…ä¸€æ¬¡å°‡æ•´æ®µå½±ç‰‡è¼‰å…¥è¨˜æ†¶é«”
            with open(filepath, 'wb') as f:
                for chunk in vid.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"âœ… å·²å„²å­˜: {filename}")

    except Exception as e:
        print(f"âš ï¸ éŒ¯èª¤ï¼š{e}")
